{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86f8959a-ebff-4bbb-b398-ef17a856a665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31m_InactiveRpcError\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1853\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n",
       "\u001B[1;32m   1852\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m attempt:\n",
       "\u001B[0;32m-> 1853\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stub\u001B[38;5;241m.\u001B[39mConfig(req, metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata())\n",
       "\u001B[1;32m   1854\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_verify_response_integrity(resp)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:277\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable.__call__\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n",
       "\u001B[1;32m    268\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n",
       "\u001B[1;32m    269\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n",
       "\u001B[1;32m    270\u001B[0m     request: Any,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    275\u001B[0m     compression: Optional[grpc\u001B[38;5;241m.\u001B[39mCompression] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[1;32m    276\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[0;32m--> 277\u001B[0m     response, ignored_call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_with_call(\n",
       "\u001B[1;32m    278\u001B[0m         request,\n",
       "\u001B[1;32m    279\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n",
       "\u001B[1;32m    280\u001B[0m         metadata\u001B[38;5;241m=\u001B[39mmetadata,\n",
       "\u001B[1;32m    281\u001B[0m         credentials\u001B[38;5;241m=\u001B[39mcredentials,\n",
       "\u001B[1;32m    282\u001B[0m         wait_for_ready\u001B[38;5;241m=\u001B[39mwait_for_ready,\n",
       "\u001B[1;32m    283\u001B[0m         compression\u001B[38;5;241m=\u001B[39mcompression,\n",
       "\u001B[1;32m    284\u001B[0m     )\n",
       "\u001B[1;32m    285\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:332\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable._with_call\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n",
       "\u001B[1;32m    329\u001B[0m call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interceptor\u001B[38;5;241m.\u001B[39mintercept_unary_unary(\n",
       "\u001B[1;32m    330\u001B[0m     continuation, client_call_details, request\n",
       "\u001B[1;32m    331\u001B[0m )\n",
       "\u001B[0;32m--> 332\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m call\u001B[38;5;241m.\u001B[39mresult(), call\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:440\u001B[0m, in \u001B[0;36m_InactiveRpcError.result\u001B[0;34m(self, timeout)\u001B[0m\n",
       "\u001B[1;32m    439\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"See grpc.Future.result.\"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 440\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:315\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001B[0;34m(new_details, request)\u001B[0m\n",
       "\u001B[1;32m    314\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 315\u001B[0m     response, call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_thunk(new_method)\u001B[38;5;241m.\u001B[39mwith_call(\n",
       "\u001B[1;32m    316\u001B[0m         request,\n",
       "\u001B[1;32m    317\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mnew_timeout,\n",
       "\u001B[1;32m    318\u001B[0m         metadata\u001B[38;5;241m=\u001B[39mnew_metadata,\n",
       "\u001B[1;32m    319\u001B[0m         credentials\u001B[38;5;241m=\u001B[39mnew_credentials,\n",
       "\u001B[1;32m    320\u001B[0m         wait_for_ready\u001B[38;5;241m=\u001B[39mnew_wait_for_ready,\n",
       "\u001B[1;32m    321\u001B[0m         compression\u001B[38;5;241m=\u001B[39mnew_compression,\n",
       "\u001B[1;32m    322\u001B[0m     )\n",
       "\u001B[1;32m    323\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _UnaryOutcome(response, call)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:1198\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable.with_call\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n",
       "\u001B[1;32m   1192\u001B[0m (\n",
       "\u001B[1;32m   1193\u001B[0m     state,\n",
       "\u001B[1;32m   1194\u001B[0m     call,\n",
       "\u001B[1;32m   1195\u001B[0m ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_blocking(\n",
       "\u001B[1;32m   1196\u001B[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n",
       "\u001B[1;32m   1197\u001B[0m )\n",
       "\u001B[0;32m-> 1198\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _end_unary_response_blocking(state, call, \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:1006\u001B[0m, in \u001B[0;36m_end_unary_response_blocking\u001B[0;34m(state, call, with_call, deadline)\u001B[0m\n",
       "\u001B[1;32m   1005\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1006\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m _InactiveRpcError(state)\n",
       "\n",
       "\u001B[0;31m_InactiveRpcError\u001B[0m: <_InactiveRpcError of RPC that terminated with:\n",
       "\tstatus = StatusCode.UNAVAILABLE\n",
       "\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:7073: Failed to connect to remote host: Timeout occurred: FD Shutdown\"\n",
       "\tdebug_error_string = \"UNKNOWN:Error received from peer  {created_time:\"2025-07-18T21:32:55.587800646+00:00\", grpc_status:14, grpc_message:\"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:7073: Failed to connect to remote host: Timeout occurred: FD Shutdown\"}\"\n",
       ">\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "\u001B[0;31mRetriesExceeded\u001B[0m                           Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-8172627868969964>, line 11\u001B[0m\n",
       "\u001B[1;32m      8\u001B[0m columns \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
       "\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Create DataFrame\u001B[39;00m\n",
       "\u001B[0;32m---> 11\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(data, columns)\n",
       "\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Write DataFrame to the Unity Catalog table: cicak_tester.default.test_table\u001B[39;00m\n",
       "\u001B[1;32m     14\u001B[0m df\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msaveAsTable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcicak_tester.default.test_table\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/session.py:657\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n",
       "\u001B[1;32m    655\u001B[0m         _schema \u001B[38;5;241m=\u001B[39m StructType()\u001B[38;5;241m.\u001B[39madd(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue\u001B[39m\u001B[38;5;124m\"\u001B[39m, _schema)\n",
       "\u001B[1;32m    656\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 657\u001B[0m     _schema \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferSchemaFromList(_data, _cols)\n",
       "\u001B[1;32m    659\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _cols \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m cast(\u001B[38;5;28mint\u001B[39m, _num_cols) \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mlen\u001B[39m(_cols):\n",
       "\u001B[1;32m    660\u001B[0m         _num_cols \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(_cols)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/session.py:432\u001B[0m, in \u001B[0;36mSparkSession._inferSchemaFromList\u001B[0;34m(self, data, names)\u001B[0m\n",
       "\u001B[1;32m    421\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data:\n",
       "\u001B[1;32m    422\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n",
       "\u001B[1;32m    423\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCANNOT_INFER_EMPTY_SCHEMA\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    424\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{},\n",
       "\u001B[1;32m    425\u001B[0m     )\n",
       "\u001B[1;32m    427\u001B[0m (\n",
       "\u001B[1;32m    428\u001B[0m     infer_dict_as_struct,\n",
       "\u001B[1;32m    429\u001B[0m     infer_array_from_first_element,\n",
       "\u001B[1;32m    430\u001B[0m     infer_map_from_first_pair,\n",
       "\u001B[1;32m    431\u001B[0m     prefer_timestamp_ntz,\n",
       "\u001B[0;32m--> 432\u001B[0m ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client\u001B[38;5;241m.\u001B[39mget_configs(\n",
       "\u001B[1;32m    433\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.pyspark.inferNestedDictAsStruct.enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    434\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.pyspark.legacy.inferArrayTypeFromFirstElement.enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    435\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.pyspark.legacy.inferMapTypeFromFirstPair.enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    436\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.timestampType\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    437\u001B[0m )\n",
       "\u001B[1;32m    438\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m functools\u001B[38;5;241m.\u001B[39mreduce(\n",
       "\u001B[1;32m    439\u001B[0m     _merge_type,\n",
       "\u001B[1;32m    440\u001B[0m     (\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    450\u001B[0m     ),\n",
       "\u001B[1;32m    451\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1819\u001B[0m, in \u001B[0;36mSparkConnectClient.get_configs\u001B[0;34m(self, *keys)\u001B[0m\n",
       "\u001B[1;32m   1817\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_configs\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mkeys: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Optional[\u001B[38;5;28mstr\u001B[39m], \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m]:\n",
       "\u001B[1;32m   1818\u001B[0m     op \u001B[38;5;241m=\u001B[39m pb2\u001B[38;5;241m.\u001B[39mConfigRequest\u001B[38;5;241m.\u001B[39mOperation(get\u001B[38;5;241m=\u001B[39mpb2\u001B[38;5;241m.\u001B[39mConfigRequest\u001B[38;5;241m.\u001B[39mGet(keys\u001B[38;5;241m=\u001B[39mkeys))\n",
       "\u001B[0;32m-> 1819\u001B[0m     configs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig(op)\u001B[38;5;241m.\u001B[39mpairs)\n",
       "\u001B[1;32m   1820\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m(configs\u001B[38;5;241m.\u001B[39mget(key) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m keys)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1858\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n",
       "\u001B[1;32m   1856\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid state during retry exception handling.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   1857\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1858\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2053\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2049\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
       "\u001B[1;32m   2050\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\n",
       "\u001B[1;32m   2051\u001B[0m                 error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNO_ACTIVE_SESSION\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m()\n",
       "\u001B[1;32m   2052\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m-> 2053\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2054\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   2055\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1851\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n",
       "\u001B[1;32m   1849\u001B[0m req\u001B[38;5;241m.\u001B[39moperation\u001B[38;5;241m.\u001B[39mCopyFrom(operation)\n",
       "\u001B[1;32m   1850\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m-> 1851\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m attempt \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrying():\n",
       "\u001B[1;32m   1852\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m attempt:\n",
       "\u001B[1;32m   1853\u001B[0m             resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stub\u001B[38;5;241m.\u001B[39mConfig(req, metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata())\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/retries.py:295\u001B[0m, in \u001B[0;36mRetrying.__iter__\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    292\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m AttemptManager(\u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_done:\n",
       "\u001B[0;32m--> 295\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait()\n",
       "\u001B[1;32m    296\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m AttemptManager(\u001B[38;5;28mself\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/retries.py:280\u001B[0m, in \u001B[0;36mRetrying._wait\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    278\u001B[0m \u001B[38;5;66;03m# Exceeded retries\u001B[39;00m\n",
       "\u001B[1;32m    279\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGiven up on retrying. error: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mrepr\u001B[39m(exception)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m--> 280\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m RetriesExceeded(error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRETRIES_EXCEEDED\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m{}) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexception\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mRetriesExceeded\u001B[0m: [RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "RetriesExceeded",
        "evalue": "[RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
       },
       "metadata": {
        "errorSummary": "[RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "RETRIES_EXCEEDED",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": null,
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31m_InactiveRpcError\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1853\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n\u001B[1;32m   1852\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m attempt:\n\u001B[0;32m-> 1853\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stub\u001B[38;5;241m.\u001B[39mConfig(req, metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata())\n\u001B[1;32m   1854\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_verify_response_integrity(resp)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:277\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable.__call__\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n\u001B[1;32m    268\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[1;32m    269\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    270\u001B[0m     request: Any,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    275\u001B[0m     compression: Optional[grpc\u001B[38;5;241m.\u001B[39mCompression] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    276\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m--> 277\u001B[0m     response, ignored_call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_with_call(\n\u001B[1;32m    278\u001B[0m         request,\n\u001B[1;32m    279\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[1;32m    280\u001B[0m         metadata\u001B[38;5;241m=\u001B[39mmetadata,\n\u001B[1;32m    281\u001B[0m         credentials\u001B[38;5;241m=\u001B[39mcredentials,\n\u001B[1;32m    282\u001B[0m         wait_for_ready\u001B[38;5;241m=\u001B[39mwait_for_ready,\n\u001B[1;32m    283\u001B[0m         compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[1;32m    284\u001B[0m     )\n\u001B[1;32m    285\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:332\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable._with_call\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n\u001B[1;32m    329\u001B[0m call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interceptor\u001B[38;5;241m.\u001B[39mintercept_unary_unary(\n\u001B[1;32m    330\u001B[0m     continuation, client_call_details, request\n\u001B[1;32m    331\u001B[0m )\n\u001B[0;32m--> 332\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m call\u001B[38;5;241m.\u001B[39mresult(), call\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:440\u001B[0m, in \u001B[0;36m_InactiveRpcError.result\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    439\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"See grpc.Future.result.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 440\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:315\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001B[0;34m(new_details, request)\u001B[0m\n\u001B[1;32m    314\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 315\u001B[0m     response, call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_thunk(new_method)\u001B[38;5;241m.\u001B[39mwith_call(\n\u001B[1;32m    316\u001B[0m         request,\n\u001B[1;32m    317\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mnew_timeout,\n\u001B[1;32m    318\u001B[0m         metadata\u001B[38;5;241m=\u001B[39mnew_metadata,\n\u001B[1;32m    319\u001B[0m         credentials\u001B[38;5;241m=\u001B[39mnew_credentials,\n\u001B[1;32m    320\u001B[0m         wait_for_ready\u001B[38;5;241m=\u001B[39mnew_wait_for_ready,\n\u001B[1;32m    321\u001B[0m         compression\u001B[38;5;241m=\u001B[39mnew_compression,\n\u001B[1;32m    322\u001B[0m     )\n\u001B[1;32m    323\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _UnaryOutcome(response, call)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:1198\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable.with_call\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n\u001B[1;32m   1192\u001B[0m (\n\u001B[1;32m   1193\u001B[0m     state,\n\u001B[1;32m   1194\u001B[0m     call,\n\u001B[1;32m   1195\u001B[0m ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_blocking(\n\u001B[1;32m   1196\u001B[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001B[1;32m   1197\u001B[0m )\n\u001B[0;32m-> 1198\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _end_unary_response_blocking(state, call, \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:1006\u001B[0m, in \u001B[0;36m_end_unary_response_blocking\u001B[0;34m(state, call, with_call, deadline)\u001B[0m\n\u001B[1;32m   1005\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1006\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m _InactiveRpcError(state)\n",
        "\u001B[0;31m_InactiveRpcError\u001B[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:7073: Failed to connect to remote host: Timeout occurred: FD Shutdown\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {created_time:\"2025-07-18T21:32:55.587800646+00:00\", grpc_status:14, grpc_message:\"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:7073: Failed to connect to remote host: Timeout occurred: FD Shutdown\"}\"\n>",
        "\nThe above exception was the direct cause of the following exception:\n",
        "\u001B[0;31mRetriesExceeded\u001B[0m                           Traceback (most recent call last)",
        "File \u001B[0;32m<command-8172627868969964>, line 11\u001B[0m\n\u001B[1;32m      8\u001B[0m columns \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Create DataFrame\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(data, columns)\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Write DataFrame to the Unity Catalog table: cicak_tester.default.test_table\u001B[39;00m\n\u001B[1;32m     14\u001B[0m df\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msaveAsTable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcicak_tester.default.test_table\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/session.py:657\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    655\u001B[0m         _schema \u001B[38;5;241m=\u001B[39m StructType()\u001B[38;5;241m.\u001B[39madd(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue\u001B[39m\u001B[38;5;124m\"\u001B[39m, _schema)\n\u001B[1;32m    656\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 657\u001B[0m     _schema \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferSchemaFromList(_data, _cols)\n\u001B[1;32m    659\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _cols \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m cast(\u001B[38;5;28mint\u001B[39m, _num_cols) \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mlen\u001B[39m(_cols):\n\u001B[1;32m    660\u001B[0m         _num_cols \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(_cols)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/session.py:432\u001B[0m, in \u001B[0;36mSparkSession._inferSchemaFromList\u001B[0;34m(self, data, names)\u001B[0m\n\u001B[1;32m    421\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data:\n\u001B[1;32m    422\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n\u001B[1;32m    423\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCANNOT_INFER_EMPTY_SCHEMA\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    424\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{},\n\u001B[1;32m    425\u001B[0m     )\n\u001B[1;32m    427\u001B[0m (\n\u001B[1;32m    428\u001B[0m     infer_dict_as_struct,\n\u001B[1;32m    429\u001B[0m     infer_array_from_first_element,\n\u001B[1;32m    430\u001B[0m     infer_map_from_first_pair,\n\u001B[1;32m    431\u001B[0m     prefer_timestamp_ntz,\n\u001B[0;32m--> 432\u001B[0m ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client\u001B[38;5;241m.\u001B[39mget_configs(\n\u001B[1;32m    433\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.pyspark.inferNestedDictAsStruct.enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    434\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.pyspark.legacy.inferArrayTypeFromFirstElement.enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    435\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.pyspark.legacy.inferMapTypeFromFirstPair.enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    436\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.timestampType\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    437\u001B[0m )\n\u001B[1;32m    438\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m functools\u001B[38;5;241m.\u001B[39mreduce(\n\u001B[1;32m    439\u001B[0m     _merge_type,\n\u001B[1;32m    440\u001B[0m     (\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    450\u001B[0m     ),\n\u001B[1;32m    451\u001B[0m )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1819\u001B[0m, in \u001B[0;36mSparkConnectClient.get_configs\u001B[0;34m(self, *keys)\u001B[0m\n\u001B[1;32m   1817\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_configs\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mkeys: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Optional[\u001B[38;5;28mstr\u001B[39m], \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m]:\n\u001B[1;32m   1818\u001B[0m     op \u001B[38;5;241m=\u001B[39m pb2\u001B[38;5;241m.\u001B[39mConfigRequest\u001B[38;5;241m.\u001B[39mOperation(get\u001B[38;5;241m=\u001B[39mpb2\u001B[38;5;241m.\u001B[39mConfigRequest\u001B[38;5;241m.\u001B[39mGet(keys\u001B[38;5;241m=\u001B[39mkeys))\n\u001B[0;32m-> 1819\u001B[0m     configs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig(op)\u001B[38;5;241m.\u001B[39mpairs)\n\u001B[1;32m   1820\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m(configs\u001B[38;5;241m.\u001B[39mget(key) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m keys)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1858\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n\u001B[1;32m   1856\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid state during retry exception handling.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1857\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1858\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2053\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2049\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n\u001B[1;32m   2050\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\n\u001B[1;32m   2051\u001B[0m                 error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNO_ACTIVE_SESSION\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m()\n\u001B[1;32m   2052\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2053\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2054\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   2055\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1851\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n\u001B[1;32m   1849\u001B[0m req\u001B[38;5;241m.\u001B[39moperation\u001B[38;5;241m.\u001B[39mCopyFrom(operation)\n\u001B[1;32m   1850\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1851\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m attempt \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrying():\n\u001B[1;32m   1852\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m attempt:\n\u001B[1;32m   1853\u001B[0m             resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stub\u001B[38;5;241m.\u001B[39mConfig(req, metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata())\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/retries.py:295\u001B[0m, in \u001B[0;36mRetrying.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    292\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m AttemptManager(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_done:\n\u001B[0;32m--> 295\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait()\n\u001B[1;32m    296\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m AttemptManager(\u001B[38;5;28mself\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/retries.py:280\u001B[0m, in \u001B[0;36mRetrying._wait\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;66;03m# Exceeded retries\u001B[39;00m\n\u001B[1;32m    279\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGiven up on retrying. error: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mrepr\u001B[39m(exception)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 280\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m RetriesExceeded(error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRETRIES_EXCEEDED\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m{}) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexception\u001B[39;00m\n",
        "\u001B[0;31mRetriesExceeded\u001B[0m: [RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Create example data\n",
    "data = [(\"Alice\", 1), (\"Bob\", 2)]\n",
    "columns = [\"name\", \"value\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Write DataFrame to the Unity Catalog table: cicak_tester.default.test_table\n",
    "df.write.mode(\"append\").saveAsTable(\"cicak_tester.default.test_table\")\n",
    "\n",
    "print(\"Table written successfully\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "FirstNotebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}