# simple_pyspark_job_with_dbx
Running a PySpark job with a DBX Notebook, and then submitting to the Jobs API


Step 1 - Create a Git Folder within Databricks (using this repository)

Step 2 - Open the Notebook (FirstNotebook.ipynb)

Change the catalog "cicak_tester" to your catalog, and the schema "default" to your schema. 

Step 3 - Run the Notebook in Serverless


-----

